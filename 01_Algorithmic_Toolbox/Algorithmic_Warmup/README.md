# ğŸš€ MÃ³dulo 2: Algorithmic Warmup (AnÃ¡lisis de Complejidad)

Este documento sirve como base teÃ³rica para entender la eficiencia algorÃ­tmica. AquÃ­ aprendemos a medir la calidad de un algoritmo mediante matemÃ¡ticas y lÃ³gica, independiente del hardware.

---

## â±ï¸ 1. La IlusiÃ³n del Tiempo Real (Runtimes)

### El problema de medir con segundos
PodrÃ­amos pensar: *"Para saber cuÃ¡l algoritmo es mejor, los corro ambos y cronometro cuÃ¡l termina primero"*.
Esto **NO** es fiable por estas razones:

1.  **Hardware variable:** Tu laptop es mÃ¡s rÃ¡pida hoy que una computadora de hace 10 aÃ±os.
2.  **Compilador:** Python, C++ y Java ejecutan la misma lÃ³gica a velocidades muy distintas.
3.  **Carga del sistema:** Si tienes muchas apps abiertas, tu algoritmo correrÃ¡ mÃ¡s lento.

### ğŸ§  Concepto Clave: JerarquÃ­a de Memoria
Uno de los factores ocultos que mÃ¡s afecta el tiempo real es **dÃ³nde estÃ¡n los datos**.
* **Registros CPU / CachÃ© L1/L2:** Es como tener la informaciÃ³n en tu mano. Extremadamente rÃ¡pido.
* **RAM:** Es como tener que levantarte a buscar un libro al estante. Lento.
* **Disco Duro:** Es como tener que ir a la biblioteca pÃºblica. Eterno.

> **ConclusiÃ³n:** No podemos confiar en el tiempo de reloj. Necesitamos una medida que sea **independiente de la mÃ¡quina**.

---

## ğŸ“ˆ 2. NotaciÃ³n AsintÃ³tica (El estÃ¡ndar de la industria)

La palabra **"AsintÃ³tica"** se refiere al comportamiento de una curva a medida que avanza al infinito.

En algoritmos, nos interesa el **comportamiento a gran escala (Scaling)**.
* No nos importa si el algoritmo es rÃ¡pido con 10 datos.
* Nos importa: **Â¿QuÃ© pasa si le metemos 1 millÃ³n de datos?** Â¿El tiempo se duplica (lineal) o explota (exponencial)?

### La falacia de las "LÃ­neas de CÃ³digo"

# Caso A: 3 lÃ­neas, pero LENTO (CuadrÃ¡tico)
for i in range(n):
    for j in range(n):
        print(i, j)  # Se ejecuta n*n veces

# Caso B: 1 lÃ­nea, pero RÃPIDO (Constante)
result = 500 * 1000  # Se ejecuta 1 vez

> **LecciÃ³n:** Un bucle for vale por $n$ operaciones. Un bucle anidado vale por $n^2$. No cuentes lÃ­neas; cuenta operaciones en funciÃ³n de la entrada $N$.

## ğŸ…¾ï¸ 3. Big-O Notation ($O$)

Big-O es la herramienta matemÃ¡tica para clasificar algoritmos segÃºn su tasa de crecimiento en el peor escenario posible (**Upper Bound**).

### ğŸ¤“ DefiniciÃ³n MatemÃ¡tica (Traducida)

$$f(n) \le c \cdot g(n)$$

*(para todo $n \ge N$)*

* **$f(n)$:** El tiempo real y "sucio" de tu algoritmo (ej. $3n^2 + 50n + 9$).
* **$g(n)$:** La versiÃ³n simplificada (ej. $n^2$).
* **$c$ (La constante):** Un "factor de ajuste". No importa si tu PC es 10 veces mÃ¡s lenta ($c=10$); la curva de crecimiento sigue siendo cuadrÃ¡tica. Big-O ignora esto.
* **$N$ (El umbral):** El punto donde el tamaÃ±o de la entrada es lo suficientemente grande como para que la clasificaciÃ³n sea vÃ¡lida.

---

## âš¡ 4. Big-O Cheatsheet: JerarquÃ­a de Poder

Ordenado de **Mejor (RÃ¡pido)** a **Peor (Lento)**.



[Image of Big O complexity chart]


| NotaciÃ³n | Nombre | AnalogÃ­a del Mundo Real | Ejemplo TÃ©cnico |
| :--- | :--- | :--- | :--- |
| $O(1)$ | Constante | Saber el nombre de la primera persona en una fila. Tardas lo mismo si hay 10 o 1 millÃ³n. | `array[index]` |
| $O(\log n)$ | LogarÃ­tmico | Buscar en una guÃ­a telefÃ³nica (o diccionario). Cortas el problema a la mitad en cada paso. | Binary Search |
| $O(n)$ | Lineal | Leer un libro pÃ¡gina por pÃ¡gina. Si el libro es el doble de grueso, tardas el doble. | Bucle `for` simple |
| $O(n \log n)$ | LinealÃ­thmico | Ordenar una baraja de cartas usando una estrategia eficiente (Merge). | Merge Sort, Quick Sort |
| $O(n^2)$ | CuadrÃ¡tico | En una fiesta, saludar a cada persona y presentarla con todas las demÃ¡s. | Bucles anidados |
| $2^n$ | Exponencial | Intentar adivinar una contraseÃ±a probando todas las combinaciones posibles. Intratable. | Fibonacci Recursivo |

---

## ğŸ§® 5. Reglas para Simplificar (Math Rules)

Usa estas reglas para encontrar el Big-O de cualquier cÃ³digo rÃ¡pidamente.

### Regla 1: El rey manda (Dominancia)
Si tu algoritmo hace varias cosas, solo importa la parte mÃ¡s lenta.

$$n^2 + n + 1000 \rightarrow O(n^2)$$

> **Por quÃ©:** Si $n = 1,000,000$, entonces $n^2$ es un billÃ³n, mientras que $n$ es solo un millÃ³n. El tÃ©rmino lineal se vuelve insignificante.

### Regla 2: Ignora las constantes

$$5n^3 \rightarrow O(n^3)$$

> **Por quÃ©:** Nos interesa la forma de la curva, no su pendiente exacta. Un algoritmo cÃºbico siempre perderÃ¡ contra uno cuadrado a la larga, sin importar las constantes.

### Regla 3: JerarquÃ­a de Clases

$$\log n < \sqrt{n} < n < n \log n < n^2 < 2^n$$

---

## ğŸš¦ 6. Diferencias: $O$, $\Omega$, $\Theta$

En entrevistas o textos acadÃ©micos verÃ¡s otros sÃ­mbolos:

* **Big-O ($O$): El Techo (Peor caso)**
    * "Mi algoritmo nunca tardarÃ¡ mÃ¡s que esto".
    * Es una garantÃ­a. Es la mÃ¡s usada en ingenierÃ­a de software.
* **Big-Omega ($\Omega$): El Suelo (Mejor caso)**
    * "Mi algoritmo tardarÃ¡ al menos esto".
    * *Ejemplo:* Para encontrar el mÃ­nimo en una lista desordenada, al menos debes ver todos los elementos una vez ($\Omega(n)$).
* **Big-Theta ($\Theta$): El Ajuste Exacto**
    * Cuando el techo y el suelo coinciden.
    * "Mi algoritmo siempre se comporta asÃ­".

> ğŸ’¡ **Tip Pro:** Si en una entrevista te preguntan la complejidad, asume que preguntan por **Big-O (Tiempo)** y **Complejidad Espacial (Memoria RAM)**.